# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYmo5BJb04eEFTX755qyrwG_oDseetga

## Part I: Data Pre-processing
"""

## Global imports
import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import os
import random
import re
import string
from gensim.models import Word2Vec, FastText
import pickle
from collections import Counter
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
##Todo 5 class import

from todo5_trainer import Todo5WordEmbeddingTrainer

## Global configuration
DATA_FILE = "questions-words.txt"
CSV_FILE = "questions-words.csv"
MODELS_DIR = "models"

def ensure_models_dir(): # Ensure the models directory exists
    
    os.makedirs(MODELS_DIR, exist_ok=True) 

# Download the Google Analogy dataset
#!wget http://download.tensorflow.org/data/questions-words.txt #operat in local

def save_model_data(model, model_name="fasttext_model", save_dir="saved_models"):
    """
    Args:
        model: 
        model_name: 
        save_dir: 
    """
    
    os.makedirs(save_dir, exist_ok=True)
    
    # save model
    model_path = os.path.join(save_dir, f"{model_name}.pkl")
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"Model saved to: {model_path}")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")
    
    return model_path


def load_saved_model(model_path="saved_models/fasttext_model.pkl"):
    
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    print(f"Model loaded from: {model_path}")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")
    
    return model

def combine_wikipedia_files():
    """
    Combine txt files into wiki_texts_combined.txt
    """
    print("Combining Wikipedia files...")
    
    with open("wiki_texts_combined.txt", "w", encoding="utf-8") as combined_file:
        for i in range(11):  # 0 to 10
            txt_file = f"wiki_texts_part_{i}.txt"
            if os.path.exists(txt_file):
                with open(txt_file, "r", encoding="utf-8") as f:
                    combined_file.write(f.read())
                print(f"Added {txt_file}")
            else:
                print(f"Warning: {txt_file} not found")
    
    # Check the combined file
    with open("wiki_texts_combined.txt", "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    print(f"Combined file created with {len(lines)} total lines")
    print("First 5 lines:")
    for i, line in enumerate(lines[:5]):
        print(f"{i+1}: {line.strip()[:100]}...")

    
    # Check the combined file
    with open("wiki_texts_combined.txt", "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    print(f"Combined file created with {len(lines)} total lines")
    print("First 5 lines:")
    for i, line in enumerate(lines[:5]):
        print(f"{i+1}: {line.strip()[:100]}...")



def todo1_process_data_to_dataframe():
    """
    Returns:
        pd.DataFrame: DataFrame with Question, Category, SubCategory columns

    """
    
    file_name = "questions-words"
    with open(f"{file_name}.txt", "r") as f:
        data = f.read().splitlines()

    # check data from the first 10 entries
    for entry in data[:10]:
        print(entry)

    # TODO1: Write your code here for processing data to pd.DataFrame
    # Please note that the first five mentions of ": " indicate `semantic`,
    # and the remaining nine belong to the `syntactic` category.
    
    questions = []
    categories = []
    sub_categories = []
    
    current_category = None
    current_sub_category = None
    semantic_count = 0
    
    for line in data:
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
        
        # Check if this is a category/subcategory header (starts with ':')
        if line.startswith(':'):
            current_sub_category = line
            
            # Determine if this is semantic or syntactic
            # First 5 categories are semantic, remaining are syntactic
            semantic_count += 1
            if semantic_count <= 5:
                current_category = "semantic"
            else:
                current_category = "syntactic"
        else:
            # This is an analogy question line
            words = line.split()
            if len(words) == 4:
                # Format: word_a word_b word_c word_d (a:b :: c:d)
                analogy_question = ' '.join(words)
                questions.append(analogy_question)
                categories.append(current_category)
                sub_categories.append(current_sub_category)

    # Create the dataframe
    df = pd.DataFrame(
        {
            "Question": questions,
            "Category": categories,
            "SubCategory": sub_categories,
        }
    )
    
    print(f"\nDataFrame Info:")
    print(f"Total questions: {len(df)}")
    print(f"Categories distribution:")
    print(df['Category'].value_counts())
    
    df.head()

    df.to_csv(f"{file_name}.csv", index=False)
    
    print(f"\nDataFrame saved to {file_name}.csv")
    
    return df


"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

def todo2_predict_with_pretrained():
    """
    Uses FastText pre-trained model to perform word analogy predictions

    """
    
    # Load the processed data
    data = pd.read_csv("questions-words.csv")

    MODEL_NAME = "fasttext-wiki-news-subwords-300"
    # You can try other models.
    # https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

    # Load the pre-trained model (using FastText vectors here)
    model = gensim.downloader.load(MODEL_NAME)
    print("The Gensim model loaded successfully!")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")

    # Do predictions and preserve the gold answers (word_D)
    preds = []
    golds = []

    for analogy in tqdm(data["Question"], desc="Processing analogies"):
        # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
        # You should also preserve the gold answers during iterations for evaluations later.
        """ Hints
        # Unpack the analogy (e.g., "man", "woman", "king", "queen")
        # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
        # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
        # Mikolov et al., 2013: big - biggest and small - smallest
        # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
        """
        
        # Unpack the analogy
        words = analogy.split()
        
        if len(words) == 4:
            word_a, word_b, word_c, word_d = words
            
            # Store the gold answer (word_d)
            golds.append(word_d)
            
            try:
                # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
                # So: vector(word_b) - vector(word_a) + vector(word_c) ≈ vector(word_d)
                # FastText can handle OOV words, so we don't need vocabulary check
                
                similar_words = model.most_similar(
                    positive=[word_b, word_c],  # Add these vectors
                    negative=[word_a],          # Subtract this vector
                    topn=1                     # Return most similar word
                )
                
                # Get the most similar word
                predicted_word = similar_words[0][0]
                preds.append(predicted_word)
                
            except Exception as e:
                # Handle any exceptions (though FastText rarely fails)
                preds.append("")
        else:
            # If the analogy doesn't have exactly 4 words, append empty prediction
            preds.append("")
            golds.append("")

    # Print basic statistics
    successful_preds = sum(1 for pred in preds if pred != "")
    print(f"\nPrediction Statistics:")
    print(f"Total questions: {len(preds)}")
    print(f"Successful predictions: {successful_preds} / {len(preds)} ({successful_preds/len(preds)*100:.2f}%)")
    print(f"Sample predictions: {[pred for pred in preds[:10] if pred != ''][:5]}")
    print(f"Sample gold answers: {golds[:10]}")

    # Perform evaluations. You do not need to modify this block!!
    def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
        return np.mean(gold == pred)

    golds_np, preds_np = np.array(golds), np.array(preds)
    
    print(f"\n=== EVALUATION RESULTS ===")
    
    # Overall accuracy
    overall_acc = calculate_accuracy(golds_np, preds_np)
    print(f"Overall Accuracy: {overall_acc * 100:.2f}%")

    # Evaluation: categories
    print(f"\nCategory-wise Accuracy:")
    for category in data["Category"].unique():
        mask = data["Category"] == category
        golds_cat, preds_cat = golds_np[mask], preds_np[mask]
        acc_cat = calculate_accuracy(golds_cat, preds_cat)
        print(f"Category: {category}, Accuracy: {acc_cat * 100:.2f}%")

    # Evaluation: sub-categories
    print(f"\nSub-category Accuracy:")
    for sub_category in data["SubCategory"].unique():
        mask = data["SubCategory"] == sub_category
        golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
        acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
        print(f"Sub-Category {sub_category}, Accuracy: {acc_subcat * 100:.2f}%")
    
    return preds, golds, model

# save the model for TODO3 usage
#model_path = save_model_data(model)
#print("Model data saved for TODO3 usage")

def todo3_plot_tsne_family():
    """
    TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
    """
    
    # Load saved model and data
    with open("saved_models/fasttext_model.pkl", 'rb') as f:
        model = pickle.load(f)
    data = pd.read_csv("questions-words.csv")
    
    # Collect words from Google Analogy dataset
    SUB_CATEGORY = ": family"
    
    # TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
    
    # Get family questions and extract words
    family_questions = data[data["SubCategory"] == SUB_CATEGORY]["Question"]
    family_words = set()
    for question in family_questions:
        words = question.split()
        if len(words) == 4:
            family_words.update(words)
    
    # Get word vectors
    word_vectors = []
    word_labels = []
    for word in family_words:
        try:
            word_vectors.append(model[word])
            word_labels.append(word)
        except KeyError:
            continue
    
    # Apply t-SNE
    vectors_matrix = np.array(word_vectors)
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_labels)-1))
    vectors_2d = tsne.fit_transform(vectors_matrix)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])
    
    for i, word in enumerate(word_labels):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 5), 
                    textcoords='offset points', fontsize=10)
    
    plt.title("Word Relationships from Google Analogy Task")
    plt.show()
    plt.savefig("word_relationships.png", bbox_inches="tight")



### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
#- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.



def todo4_sample_wikipedia(sample_ratio=0.2):
    """
    TODO4: Sample 20% Wikipedia articles
    using wiki_sample.txt for trianing (20% Data)
    """
    print(f"TODO4: Sampling {sample_ratio*100}% of Wikipedia articles")
    
    random.seed(42)  # For reproducibility
    
    wiki_txt_path = "wiki_texts_combined.txt"
    output_path = "wiki_sample.txt"
    
    with open(wiki_txt_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # TODO4: Sample 20% Wikipedia articles
    total_lines = len(lines)
    sample_size = int(total_lines * sample_ratio)
    sampled_lines = random.sample(lines, sample_size)
    
    with open(output_path, "w", encoding="utf-8") as output_file:
        output_file.writelines(sampled_lines)
    
    print(f"Original articles: {total_lines:,}")
    print(f"Sampled articles: {len(sampled_lines):,}")
    print(f"Sample saved to: {output_path}")
    
    return output_path


# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
# See the code in todo5_trainer.py



def todo6_predict_with_custom_embeddings():
    """
    TODO6: Use trained word embeddings for analogy predictions
    """
    
    from sklearn.manifold import TSNE
    
    # Load data
    data = pd.read_csv("questions-words.csv")
    
    # Load trained models
    word2vec_model = Word2Vec.load("models/word2vec_optimized.model")
    fasttext_model = FastText.load("models/fasttext_optimized.model")
    
    results = {}
    
    for model_name, model in [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]:
        preds = []
        golds = []
        
        for analogy in tqdm(data["Question"], desc=f"Processing {model_name}"):
            words = analogy.split()
            if len(words) == 4:
                word_a, word_b, word_c, word_d = words
                golds.append(word_d)
                
                try:
                    # Check if words in vocabulary (Word2Vec will fail, FastText won't)
                    if model_name == "Word2Vec":
                        if all(w in model.wv for w in [word_a, word_b, word_c]):
                            result = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
                            preds.append(result[0][0])
                        else:
                            preds.append("")
                    else:  # FastText can handle OOV
                        result = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
                        preds.append(result[0][0])
                except:
                    preds.append("")
            else:
                preds.append("")
                golds.append("")
        
        # Calculate accuracy
        golds_np, preds_np = np.array(golds), np.array(preds)
        overall_acc = np.mean(golds_np == preds_np)
        
        results[model_name] = {
            'predictions': preds,
            'golds': golds,
            'accuracy': overall_acc
        }
        
        print(f"\n{model_name} Overall Accuracy: {overall_acc * 100:.2f}%")
        
        # Category-wise accuracy
        for category in data["Category"].unique():
            mask = data["Category"] == category
            acc = np.mean(golds_np[mask] == preds_np[mask])
            print(f"  {category}: {acc * 100:.2f}%")
    
    return results

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`


def todo7_plot_tsne_custom_embeddings():
    """
    TODO7: Plot t-SNE for family words using custom embeddings
    """
    # Load models
    word2vec_model = Word2Vec.load("models/word2vec_optimized.model")
    fasttext_model = FastText.load("models/fasttext_optimized.model")
    
    # Load data
    data = pd.read_csv("questions-words.csv")
    SUB_CATEGORY = ": family"
    
    # Get family words
    family_questions = data[data["SubCategory"] == SUB_CATEGORY]["Question"]
    family_words = set()
    for question in family_questions:
        words = question.split()
        if len(words) == 4:
            family_words.update(words)
    
    # Create plots for both models
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    for ax, (model_name, model) in zip([ax1, ax2], [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]):
        word_vectors = []
        word_labels = []
        
        for word in family_words:
            try:
                if model_name == "Word2Vec" and word not in model.wv:
                    continue
                word_vectors.append(model.wv[word])
                word_labels.append(word)
            except:
                continue
        
        if len(word_vectors) > 2:
            vectors_matrix = np.array(word_vectors)
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_labels)-1))
            vectors_2d = tsne.fit_transform(vectors_matrix)
            
            ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1])
            for i, word in enumerate(word_labels):
                ax.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                          xytext=(5, 5), textcoords='offset points', fontsize=9)
            
            ax.set_title(f"{model_name} - Family Word Relationships")
            ax.set_xlabel("t-SNE dimension 1")
            ax.set_ylabel("t-SNE dimension 2")
    
    plt.suptitle("Word Relationships from Google Analogy Task")
    plt.tight_layout()
    plt.savefig("word_relationships_custom.png", bbox_inches="tight")
    plt.show()
    
    print(f"Plot saved as word_relationships_custom.png")

def run_batch_experiments():#change ur sample ratio here
    
    
    # test ratios
    sample_ratios = [0.05, 0.1, 0.2, 0.3]
    
    # test result
    all_results = {}
    
    for ratio in sample_ratios:
        print("\n" + "="*70)
        print(f" EXPERIMENT: Sample Ratio = {ratio} ({ratio*100}% of data) ")
        print("="*70)
        
        # clean old file
        files_to_remove = [
            "wiki_sample.txt",
            "wiki_word2vec_corpus.txt", 
            "wiki_fasttext_corpus.txt",
            "models/word2vec_optimized.model",
            "models/fasttext_optimized.model"
        ]
        
        for file in files_to_remove:
            if os.path.exists(file):
                os.remove(file)
        
        # load config from todo5 class
        config_override = {'sample_ratio': ratio}
        
        # train 
        trainer = Todo5WordEmbeddingTrainer(config_override)
        trainer.run_complete_training()
        
        # evaluate
        print(f"\nEvaluating models for ratio={ratio}...")
        results = todo6_predict_with_custom_embeddings()
        
        
        data = pd.read_csv("questions-words.csv")
        
        # calculate acc
        categories_acc = {}
        for model_name in ['Word2Vec', 'FastText']:
            categories_acc[model_name] = {}
            preds = np.array(results[model_name]['predictions'])
            golds = np.array(results[model_name]['golds'])
            
            # Overall
            categories_acc[model_name]['overall'] = results[model_name]['accuracy']
            
            # By category
            for category in data["Category"].unique():
                mask = data["Category"] == category
                acc = np.mean(golds[mask] == preds[mask])
                categories_acc[model_name][category] = acc
        
        # save result
        all_results[f"ratio_{ratio}"] = {
            'sample_ratio': ratio,
            'word2vec_accuracy': results['Word2Vec']['accuracy'],
            'fasttext_accuracy': results['FastText']['accuracy'],
            'detailed_accuracy': categories_acc
        }
        
        print(f"\nResults for ratio {ratio}:")
        print(f"  Word2Vec: {results['Word2Vec']['accuracy']*100:.2f}%")
        print(f"  FastText: {results['FastText']['accuracy']*100:.2f}%")
    
    # save and plot
    save_and_plot_separate_charts(all_results)
    
    return all_results


def save_and_plot_separate_charts(results):
    """
    save and plot
    """
    
    # save Json
    filename = f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    print(f"\nResults saved to {filename}")
    
    # preapre data
    ratios = []
    word2vec_overall = []
    word2vec_semantic = []
    word2vec_syntactic = []
    fasttext_overall = []
    fasttext_semantic = []
    fasttext_syntactic = []
    
    for key in sorted(results.keys()):
        data = results[key]
        ratios.append(f"{data['sample_ratio']*100}%")
        
        # Word2Vec
        word2vec_overall.append(data['detailed_accuracy']['Word2Vec']['overall'] * 100)
        word2vec_semantic.append(data['detailed_accuracy']['Word2Vec']['semantic'] * 100)
        word2vec_syntactic.append(data['detailed_accuracy']['Word2Vec']['syntactic'] * 100)
        
        # FastText
        fasttext_overall.append(data['detailed_accuracy']['FastText']['overall'] * 100)
        fasttext_semantic.append(data['detailed_accuracy']['FastText']['semantic'] * 100)
        fasttext_syntactic.append(data['detailed_accuracy']['FastText']['syntactic'] * 100)
    
    # set plt parameter
    x = np.arange(len(ratios))
    width = 0.25
    
    # creat 2 plot
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))
    
    # Word2Vec plot
    bars1 = ax1.barh(x - width, word2vec_overall, width, label='Overall', color='#2E86AB')
    bars2 = ax1.barh(x, word2vec_semantic, width, label='Semantic', color='#A23B72')
    bars3 = ax1.barh(x + width, word2vec_syntactic, width, label='Syntactic', color='#F18F01')
    
    ax1.set_xlabel('Accuracy (%)', fontsize=11)
    ax1.set_title('Word2Vec Performance by Sample Ratio', fontsize=12, fontweight='bold')
    ax1.set_yticks(x)
    ax1.set_yticklabels(ratios)
    ax1.set_ylabel('Sample Ratio', fontsize=11)
    ax1.legend(loc='lower right')
    ax1.grid(axis='x', alpha=0.3)
    
    # lable data
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            width_val = bar.get_width()
            ax1.text(width_val + 0.5, bar.get_y() + bar.get_height()/2, 
                    f'{width_val:.1f}%', ha='left', va='center', fontsize=9)
    
    # FastText plot
    bars4 = ax2.barh(x - width, fasttext_overall, width, label='Overall', color='#2E86AB')
    bars5 = ax2.barh(x, fasttext_semantic, width, label='Semantic', color='#A23B72')
    bars6 = ax2.barh(x + width, fasttext_syntactic, width, label='Syntactic', color='#F18F01')
    
    ax2.set_xlabel('Accuracy (%)', fontsize=11)
    ax2.set_title('FastText Performance by Sample Ratio', fontsize=12, fontweight='bold')
    ax2.set_yticks(x)
    ax2.set_yticklabels(ratios)
    ax2.set_ylabel('Sample Ratio', fontsize=11)
    ax2.legend(loc='lower right')
    ax2.grid(axis='x', alpha=0.3)
    
    # label data
    for bars in [bars4, bars5, bars6]:
        for bar in bars:
            width_val = bar.get_width()
            ax2.text(width_val + 0.5, bar.get_y() + bar.get_height()/2, 
                    f'{width_val:.1f}%', ha='left', va='center', fontsize=9)
    
    plt.suptitle('Model Performance Comparison: Impact of Training Data Size', 
                fontsize=14, fontweight='bold', y=1.02)
    plt.tight_layout()
    plt.savefig('model_comparison_separate.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # creat plot individual
    create_individual_charts(results)
    
    # print information
    print("\n" + "="*60)
    print("EXPERIMENT SUMMARY")
    print("="*60)
    print(f"{'Ratio':<10} {'Model':<10} {'Overall':<12} {'Semantic':<12} {'Syntactic':<12}")
    print("-"*58)
    
    for i, ratio in enumerate([0.05, 0.1]):
        ratio_str = f"{ratio*100}%"
        print(f"{ratio_str:<10} {'Word2Vec':<10} {word2vec_overall[i]:<12.2f}% {word2vec_semantic[i]:<12.2f}% {word2vec_syntactic[i]:<12.2f}%")
        print(f"{'':<10} {'FastText':<10} {fasttext_overall[i]:<12.2f}% {fasttext_semantic[i]:<12.2f}% {fasttext_syntactic[i]:<12.2f}%")
        if i < len(word2vec_overall) - 1:
            print()


def create_individual_charts(results):
    
    
    # prepare data
    ratios_labels = []
    word2vec_data = {'overall': [], 'semantic': [], 'syntactic': []}
    fasttext_data = {'overall': [], 'semantic': [], 'syntactic': []}
    
    for key in sorted(results.keys()):
        data = results[key]
        ratios_labels.append(f"{data['sample_ratio']*100}%")
        
        # Word2Vec
        word2vec_data['overall'].append(data['detailed_accuracy']['Word2Vec']['overall'] * 100)
        word2vec_data['semantic'].append(data['detailed_accuracy']['Word2Vec']['semantic'] * 100)
        word2vec_data['syntactic'].append(data['detailed_accuracy']['Word2Vec']['syntactic'] * 100)
        
        # FastText
        fasttext_data['overall'].append(data['detailed_accuracy']['FastText']['overall'] * 100)
        fasttext_data['semantic'].append(data['detailed_accuracy']['FastText']['semantic'] * 100)
        fasttext_data['syntactic'].append(data['detailed_accuracy']['FastText']['syntactic'] * 100)
    
    x = np.arange(len(ratios_labels))
    width = 0.25
    
    # Word2Vec plot
    fig1, ax1 = plt.subplots(figsize=(10, 6))
    bars1 = ax1.barh(x - width, word2vec_data['overall'], width, label='Overall', color='#1f77b4')
    bars2 = ax1.barh(x, word2vec_data['semantic'], width, label='Semantic', color='#ff7f0e')
    bars3 = ax1.barh(x + width, word2vec_data['syntactic'], width, label='Syntactic', color='#2ca02c')
    
    ax1.set_xlabel('Accuracy (%)', fontsize=12)
    ax1.set_title('Word2Vec Performance Analysis', fontsize=14, fontweight='bold')
    ax1.set_yticks(x)
    ax1.set_yticklabels(ratios_labels)
    ax1.set_ylabel('Sample Ratio', fontsize=12)
    ax1.legend(loc='lower right', fontsize=11)
    ax1.grid(axis='x', alpha=0.3)
    
    for bars in [bars1, bars2, bars3]:
        for bar in bars:
            width_val = bar.get_width()
            ax1.text(width_val + 0.3, bar.get_y() + bar.get_height()/2, 
                    f'{width_val:.1f}%', ha='left', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('word2vec_performance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # FastText plot
    fig2, ax2 = plt.subplots(figsize=(10, 6))
    bars4 = ax2.barh(x - width, fasttext_data['overall'], width, label='Overall', color='#1f77b4')
    bars5 = ax2.barh(x, fasttext_data['semantic'], width, label='Semantic', color='#ff7f0e')
    bars6 = ax2.barh(x + width, fasttext_data['syntactic'], width, label='Syntactic', color='#2ca02c')
    
    ax2.set_xlabel('Accuracy (%)', fontsize=12)
    ax2.set_title('FastText Performance Analysis', fontsize=14, fontweight='bold')
    ax2.set_yticks(x)
    ax2.set_yticklabels(ratios_labels)
    ax2.set_ylabel('Sample Ratio', fontsize=12)
    ax2.legend(loc='lower right', fontsize=11)
    ax2.grid(axis='x', alpha=0.3)
    
    for bars in [bars4, bars5, bars6]:
        for bar in bars:
            width_val = bar.get_width()
            ax2.text(width_val + 0.3, bar.get_y() + bar.get_height()/2, 
                    f'{width_val:.1f}%', ha='left', va='center', fontsize=10)
    
    plt.tight_layout()
    plt.savefig('fasttext_performance.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    print("\nIndividual charts saved:")
    print("  - word2vec_performance.png")
    print("  - fasttext_performance.png")
    print("  - model_comparison_separate.png")
    
    #ave and compar below
    
    
    from datetime import datetime
    
    # Save JSON
    filename = f"batch_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(filename, 'w') as f:
        json.dump(results, f, indent=2, default=str)
    print(f"\nResults saved to {filename}")
    
    # prepare data
    ratios = []
    word2vec_acc = []
    fasttext_acc = []
    
    for key in sorted(results.keys()):
        data = results[key]
        ratios.append(data['sample_ratio'])
        word2vec_acc.append(data['word2vec_accuracy'] * 100)
        fasttext_acc.append(data['fasttext_accuracy'] * 100)
    
    # plot
    plt.figure(figsize=(10, 6))
    plt.plot(ratios, word2vec_acc, 'o-', label='Word2Vec', linewidth=2, markersize=8)
    plt.plot(ratios, fasttext_acc, 's-', label='FastText', linewidth=2, markersize=8)
    
    plt.xlabel('Sample Ratio', fontsize=12)
    plt.ylabel('Accuracy (%)', fontsize=12)
    plt.title('Model Accuracy vs Training Data Size', fontsize=14)
    plt.legend(fontsize=11)
    plt.grid(True, alpha=0.3)
    
    # lable data
    for r, w, f in zip(ratios, word2vec_acc, fasttext_acc):
        plt.annotate(f'{w:.1f}%', (r, w), textcoords="offset points", 
                    xytext=(0,10), ha='center', fontsize=9)
        plt.annotate(f'{f:.1f}%', (r, f), textcoords="offset points", 
                    xytext=(0,-15), ha='center', fontsize=9)
    
    plt.savefig('sample_ratio_comparison.png', dpi=300, bbox_inches='tight')
    plt.show()
    
    # print
    print("\n" + "="*60)
    print("EXPERIMENT SUMMARY")
    print("="*60)
    print(f"{'Sample Ratio':<15} {'Word2Vec':<15} {'FastText':<15}")
    print("-"*45)
    for r, w, f in zip(ratios, word2vec_acc, fasttext_acc):
        print(f"{r:<15.2f} {w:<15.2f}% {f:<15.2f}%")

def evaluate_custom_models_detailed():
    """
    Evaluate custom trained models with file existence check
    """
    
    
    print("\n" + "="*70)
    print(" DETAILED EVALUATION: CUSTOM MODELS ")
    print("="*70)
    
    # Check if models exist
    word2vec_path = "models/word2vec_optimized.model"
    fasttext_path = "models/fasttext_optimized.model"
    
    if not os.path.exists(word2vec_path):
        print(f"\n Error: Word2Vec model not found at {word2vec_path}")
        print("Please run TODO5 training first:")
        print("  trainer = Todo5WordEmbeddingTrainer()")
        print("  trainer.run_complete_training()")
        return None
    
    if not os.path.exists(fasttext_path):
        print(f"\n Error: FastText model not found at {fasttext_path}")
        print("Please run TODO5 training first")
        return None
    
    print("✓ Models found, loading...")
    
    # Load data
    data = pd.read_csv("questions-words.csv")
    
    # Load custom trained models
    word2vec_model = Word2Vec.load(word2vec_path)
    fasttext_model = FastText.load(fasttext_path)
    
    print(f"✓ Word2Vec loaded: {len(word2vec_model.wv):,} words")
    print(f"✓ FastText loaded: {len(fasttext_model.wv):,} words")
    
    results = {}
    
    for model_name, model in [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]:
        print(f"\n{'='*50}")
        print(f" Evaluating {model_name}")
        print(f"{'='*50}")
        
        preds = []
        golds = []
        
        # Process all analogies
        for analogy in tqdm(data["Question"], desc=f"Processing {model_name}"):
            words = analogy.split()
            if len(words) == 4:
                word_a, word_b, word_c, word_d = words
                golds.append(word_d)
                
                try:
                    if model_name == "Word2Vec":
                        # Check if all words in vocabulary
                        if all(w in model.wv for w in [word_a, word_b, word_c]):
                            result = model.wv.most_similar(
                                positive=[word_b, word_c], 
                                negative=[word_a], 
                                topn=1
                            )
                            preds.append(result[0][0])
                        else:
                            preds.append("")
                    else:  # FastText can handle OOV
                        result = model.wv.most_similar(
                            positive=[word_b, word_c], 
                            negative=[word_a], 
                            topn=1
                        )
                        preds.append(result[0][0])
                except:
                    preds.append("")
            else:
                preds.append("")
                golds.append("")
        
        # Convert to numpy arrays
        golds_np = np.array(golds)
        preds_np = np.array(preds)
        
        # Calculate overall accuracy
        overall_acc = np.mean(golds_np == preds_np)
        print(f"\n=== EVALUATION RESULTS ===")
        print(f"Overall Accuracy: {overall_acc * 100:.2f}%")
        
        # Category-wise accuracy (semantic vs syntactic)
        print(f"\nCategory-wise Accuracy:")
        category_results = {}
        for category in data["Category"].unique():
            mask = data["Category"] == category
            acc = np.mean(golds_np[mask] == preds_np[mask])
            category_results[category] = acc
            print(f"Category: {category}, Accuracy: {acc * 100:.2f}%")
        
        # Sub-category accuracy (detailed)
        print(f"\nSub-category Accuracy:")
        subcategory_results = {}
        for sub_category in data["SubCategory"].unique():
            mask = data["SubCategory"] == sub_category
            acc = np.mean(golds_np[mask] == preds_np[mask])
            subcategory_results[sub_category] = acc
            print(f"Sub-Category {sub_category}, Accuracy: {acc * 100:.2f}%")
        
        # Store results
        results[model_name] = {
            'predictions': preds,
            'golds': golds,
            'overall_accuracy': overall_acc,
            'category_accuracy': category_results,
            'subcategory_accuracy': subcategory_results
        }
    
    # Compare models
    print("\n" + "="*70)
    print(" MODEL COMPARISON ")
    print("="*70)
    
    print("\nOverall Accuracy:")
    print(f"  Word2Vec: {results['Word2Vec']['overall_accuracy']*100:.2f}%")
    print(f"  FastText: {results['FastText']['overall_accuracy']*100:.2f}%")
    
    return results


def create_evaluation_report(results):
    """
    Create a detailed evaluation report as DataFrame
    """

    
    # Create comparison dataframe
    comparison_data = []
    
    for model_name in ['Word2Vec', 'FastText']:
        model_results = results[model_name]
        
        # Add overall
        comparison_data.append({
            'Model': model_name,
            'Category': 'OVERALL',
            'SubCategory': 'ALL',
            'Accuracy': model_results['overall_accuracy'] * 100
        })
        
        # Add by category
        for cat, acc in model_results['category_accuracy'].items():
            comparison_data.append({
                'Model': model_name,
                'Category': cat,
                'SubCategory': 'ALL',
                'Accuracy': acc * 100
            })
        
        # Add by subcategory
        for subcat, acc in model_results['subcategory_accuracy'].items():
            # Determine parent category
            parent_cat = 'semantic' if 'capital' in subcat or 'currency' in subcat or 'city' in subcat or 'family' in subcat else 'syntactic'
            comparison_data.append({
                'Model': model_name,
                'Category': parent_cat,
                'SubCategory': subcat,
                'Accuracy': acc * 100
            })
    
    df = pd.DataFrame(comparison_data)
    
    # Save to CSV
    df.to_csv('evaluation_report_detailed.csv', index=False)
    print("\nDetailed evaluation report saved to 'evaluation_report_detailed.csv'")
    
    # Create pivot table for easy comparison
    pivot = df.pivot_table(
        values='Accuracy',
        index=['Category', 'SubCategory'],
        columns='Model',
        aggfunc='first'
    )
    
    print("\nModel Comparison Pivot Table:")
    print(pivot.round(2))
    
    return df, pivot

def test_word_similarity_30percent():
    """
    Test word similarity using the 30% data ratio trained models
    
    """
    
    
    print("\n" + "="*70)
    print(" WORD SIMILARITY TEST (30% Data Ratio Models) ")
    print("="*70)
    
    # Check if 30% models exist
    word2vec_path = "models/word2vec_optimized.model"
    fasttext_path = "models/fasttext_optimized.model"
    
    if not os.path.exists(word2vec_path) or not os.path.exists(fasttext_path):
        print(" Error: Models not found. Please ensure you've trained with 30% data ratio.")
        return None
    
    # Load models
    print("\nLoading models...")
    word2vec_model = Word2Vec.load(word2vec_path)
    fasttext_model = FastText.load(fasttext_path)
    
    print(f"✓ Word2Vec loaded: {len(word2vec_model.wv):,} words")
    print(f"✓ FastText loaded: {len(fasttext_model.wv):,} words")
    
    # Test words covering different categories
    test_words = [
        # Common words (should work well)
        'king', 'queen', 'man', 'woman',
        
        # Geographic words (problematic based on your results)
        'paris', 'london', 'france', 'england',
        
        # Comparative words (syntactic)
        'good', 'better', 'bad', 'worse',
        
        # Action words
        'running', 'walking', 'eating', 'drinking',
        
        # Technology words
        'computer', 'software', 'internet', 'data'
    ]
    
    results = {}
    
    for model_name, model in [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]:
        print(f"\n{'='*50}")
        print(f" {model_name} Similarity Results")
        print(f"{'='*50}")
        
        model_results = {}
        
        for word in test_words:
            try:
                if model_name == "Word2Vec":
                    # Word2Vec needs exact match
                    if word in model.wv:
                        similar = model.wv.most_similar(word, topn=5)
                        model_results[word] = similar
                        print(f"\n'{word}' most similar words:")
                        for sim_word, score in similar:
                            print(f"  → {sim_word:20s} (score: {score:.4f})")
                    else:
                        print(f"\n'{word}':  NOT IN VOCABULARY")
                        model_results[word] = "OOV"
                else:
                    # FastText can handle any word
                    similar = model.wv.most_similar(word, topn=5)
                    model_results[word] = similar
                    print(f"\n'{word}' most similar words:")
                    for sim_word, score in similar:
                        print(f"  → {sim_word:20s} (score: {score:.4f})")
                        
            except Exception as e:
                print(f"\n'{word}': Error - {str(e)}")
                model_results[word] = f"Error: {str(e)}"
        
        results[model_name] = model_results
    
    # Comparative Analysis
    print("\n" + "="*70)
    print(" COMPARATIVE ANALYSIS ")
    print("="*70)
    
    # Check OOV rate
    word2vec_oov = sum(1 for r in results['Word2Vec'].values() if r == "OOV")
    print(f"\nOut-of-Vocabulary Words:")
    print(f"  Word2Vec: {word2vec_oov}/{len(test_words)} words missing")
    print(f"  FastText: 0/{len(test_words)} (handles all words via subwords)")
    
    # Quality observations
    print("1. Geographic terms (paris, london) - Check if they appear and with what similarity")
    print("2. Semantic relationships (king-queen, man-woman) - Should show gender pairs")
    print("3. Syntactic patterns (good-better, bad-worse) - Should show comparative forms")
    print("4. Domain clusters (computer-software-internet) - Should group together")
    
    # Save detailed results to CSV
    save_similarity_results_to_csv(results, test_words)
    
    return results



    """
    Save similarity results to CSV for detailed analysis
    """
    import pandas as pd
    
    data = []
    
    for word in test_words:
        for model_name in ['Word2Vec', 'FastText']:
            model_result = results[model_name][word]
            
            if model_result == "OOV":
                data.append({
                    'Test_Word': word,
                    'Model': model_name,
                    'Rank': 0,
                    'Similar_Word': 'OUT_OF_VOCABULARY',
                    'Score': 0.0
                })
            elif isinstance(model_result, list):
                for rank, (sim_word, score) in enumerate(model_result, 1):
                    data.append({
                        'Test_Word': word,
                        'Model': model_name,
                        'Rank': rank,
                        'Similar_Word': sim_word,
                        'Score': score
                    })
    
    df = pd.DataFrame(data)
    df.to_csv('similarity_results_30percent.csv', index=False)
    print("\n✓ Detailed results saved to 'similarity_results_30percent.csv'")
    
    # Create summary statistics
    summary = df.groupby(['Model', 'Test_Word'])['Score'].mean().reset_index()
    summary.to_csv('similarity_summary_30percent.csv', index=False)
    print("✓ Summary statistics saved to 'similarity_summary_30percent.csv'")


    from gensim.models import FastText
    model = FastText.load("models/fasttext_optimized.model")
    
    test_words = ['king', 'paris', 'good', 'running', 'computer']
    
    for word in test_words:
        if word in model.wv:
            similar = model.wv.most_similar(word, topn=5)
            print(f"\n{word}:")
            for w, score in similar:
                print(f"  {w}: {score:.3f}")

if __name__ == "__main__":
    
    # TODO1
    #df = todo1_process_data_to_dataframe()
    
    # TODO2
    '''
    preds, golds, model = todo2_predict_with_pretrained()
    
    # save the model for TODO3 usage
    model_path = save_model_data(model)
    print("Model data saved for TODO3 usage")
    '''
    
    # TODO3
    #todo3_plot_tsne_family()

    # TODO4
    '''
    combine_wikipedia_files()
    # Sample 20% of the data
    sample_path = todo4_sample_wikipedia()
    '''
    
    # TODO5 - Train custom word embeddings
    # import class from todo5_trainer.py
    '''
    print("\n" + "="*70)
    print(" TODO5: TRAINING CUSTOM WORD EMBEDDINGS ")
    print("="*70)
    
    # Quick start with default settings
    # See the detel in todo_trainer.py
    trainer = Todo5WordEmbeddingTrainer()
    trainer.run_complete_training()

    print("\n" + "="*70)
    print(" TODO5 COMPLETED ")
    print("="*70)
    '''
    #Todo - 6 predict by train model
    '''
    print("\n" + "="*70)
    print(" TODO6: PREDICTIONS WITH CUSTOM EMBEDDINGS ")
    print("="*70)
    
    results = todo6_predict_with_custom_embeddings()

    '''
    
    #Todo - 7 plot T-SNE
    '''
    print("\n" + "="*70)
    print(" TODO7: t-SNE VISUALIZATION WITH CUSTOM EMBEDDINGS ")
    print("="*70)
    
    todo7_plot_tsne_custom_embeddings()
    
    print("\n" + "="*70)
    print(" ALL TASKS COMPLETED ")
    print("="*70)
    '''
    #Batch experience : start it before you change ur sample ratio which u want to test.
    '''
    results = run_batch_experiments()

    #evalute result in Todo 5
    results = evaluate_custom_models_detailed()
    
    
    df, pivot = create_evaluation_report(results)

    #Simple most similar texts evaluate for question 4
    similarity_results = test_word_similarity_30percent()
    '''
