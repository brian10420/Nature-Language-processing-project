(NLP) D:\NCUE\Yang_LAB\Brian\NLP>uv run main.py

======================================================================
 EXPERIMENT: Sample Ratio = 0.05 (5.0% of data)
======================================================================
WordEmbeddingTrainer initialized with config:
  Vector size: 300
  Workers: 20
  Epochs: 5

============================================================
Starting Complete Training Pipeline
============================================================

Step 1: Data Preparation
Sampling 5.0% of Wikipedia articles...
Sampled 281,182 lines from 5,623,655 total lines

Step 2: Training Word2Vec

============================================================
Training Word2Vec Model
============================================================
Sampled wiki file already exists (281,182 lines)
Preparing corpus for WORD2VEC...
Processing for word2vec: 281182it [00:49, 5677.71it/s]
Corpus prepared: 281,182 sentences in 49.59 seconds

Training Word2Vec with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Corpus file: wiki_word2vec_corpus.txt

Word2Vec Training Complete:
  Training time: 1463.37 seconds
  Vocabulary size: 453,177
  Model saved to: models/word2vec_optimized.model

Sample Analogy Tests for WORD2VEC:
  man:woman :: king:? -> ['queen', 'monarch', 'saovabha']
  paris:france :: london:? -> ['england', 'britain', 'yorkshire']
  good:better :: bad:? -> ['worse', 'bigger', 'crazier']

Step 3: Training FastText

============================================================
Training FastText Model
============================================================
Sampled wiki file already exists (281,182 lines)
Preparing corpus for FASTTEXT...
Processing for fasttext: 281182it [00:33, 8470.99it/s] 
Corpus prepared: 281,182 sentences in 33.22 seconds

Training FastText with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Character n-grams: 3-6
  Corpus file: wiki_fasttext_corpus.txt

FastText Training Complete:
  Training time: 2355.09 seconds
  Vocabulary size: 453,200
  Model saved to: models/fasttext_optimized.model

Sample Analogy Tests for FASTTEXT:
  man:woman :: king:? -> ['queen', 'princess', 'monarch']
  paris:france :: london:? -> ['england', 'londonlondon', 'britain']
  good:better :: bad:? -> ['betters', 'bettering', 'bettered']

Step 4: Model Comparison

============================================================
Model Comparison
============================================================
         Training Time (s) Vocabulary Size Vector Size Epochs
word2vec           1463.37         453,177         300      5
fasttext           2355.09         453,200         300      5

Out-of-Vocabulary (OOV) Handling Test:
  'xyzabc123': Word2Vec=False, FastText=True
  'unknownword999': Word2Vec=False, FastText=True
  'testingoov': Word2Vec=False, FastText=True

Step 5: Saving Results

Experiment results saved to: experiments/experiment_20250917_132544.json

============================================================
Total Training Pipeline Completed in 4938.39 seconds
  Word2Vec: 1463.37s
  FastText: 2355.09s
============================================================

Evaluating models for ratio=0.05...
Processing Word2Vec: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [03:26<00:00, 94.43it/s]

Word2Vec Overall Accuracy: 21.85%
  semantic: 4.45%
  syntactic: 36.31%
Processing FastText: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [06:44<00:00, 48.27it/s] 

FastText Overall Accuracy: 28.05%
  semantic: 3.22%
  syntactic: 48.68%

Results for ratio 0.05:
  Word2Vec: 21.85%
  FastText: 28.05%

======================================================================
 EXPERIMENT: Sample Ratio = 0.1 (10.0% of data)
======================================================================
WordEmbeddingTrainer initialized with config:
  Vector size: 300
  Workers: 20
  Epochs: 5

============================================================
Starting Complete Training Pipeline
============================================================

Step 1: Data Preparation
Sampling 10.0% of Wikipedia articles...
Sampled 562,365 lines from 5,623,655 total lines

Step 2: Training Word2Vec

============================================================
Training Word2Vec Model
============================================================
Sampled wiki file already exists (562,365 lines)
Preparing corpus for WORD2VEC...
Processing for word2vec: 562365it [01:38, 5704.40it/s]
Corpus prepared: 562,365 sentences in 98.63 seconds

Training Word2Vec with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Corpus file: wiki_word2vec_corpus.txt

Word2Vec Training Complete:
  Training time: 3129.53 seconds
  Vocabulary size: 704,697
  Model saved to: models/word2vec_optimized.model

Sample Analogy Tests for WORD2VEC:
  man:woman :: king:? -> ['queen', 'londonson', 'kekāuluohi']
  paris:france :: london:? -> ['england', 'britain', 'baginton']
  good:better :: bad:? -> ['worse', 'aibling', 'kösen']

Step 3: Training FastText

============================================================
Training FastText Model
============================================================
Sampled wiki file already exists (562,365 lines)
Preparing corpus for FASTTEXT...
Processing for fasttext: 562365it [01:07, 8347.43it/s] 
Corpus prepared: 562,365 sentences in 67.39 seconds

Training FastText with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Character n-grams: 3-6
  Corpus file: wiki_fasttext_corpus.txt

FastText Training Complete:
  Training time: 4929.68 seconds
  Vocabulary size: 704,718
  Model saved to: models/fasttext_optimized.model

Sample Analogy Tests for FASTTEXT:
  man:woman :: king:? -> ['queen', 'princess', 'dequeen']
  paris:france :: london:? -> ['londonlondon', 'england', 'rfclondon']
  good:better :: bad:? -> ['aibling', 'betters', 'retter']

Step 4: Model Comparison

============================================================
Model Comparison
============================================================
         Training Time (s) Vocabulary Size Vector Size Epochs
word2vec           3129.53         704,697         300      5
fasttext           4929.68         704,718         300      5

Out-of-Vocabulary (OOV) Handling Test:
  'xyzabc123': Word2Vec=False, FastText=True
  'unknownword999': Word2Vec=False, FastText=True
  'testingoov': Word2Vec=False, FastText=True

Step 5: Saving Results

Experiment results saved to: experiments/experiment_20250917_145910.json

============================================================
Total Training Pipeline Completed in 9388.11 seconds
  Word2Vec: 3129.53s
  FastText: 4929.68s
============================================================

Evaluating models for ratio=0.1...
Processing Word2Vec: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [05:18<00:00, 61.33it/s]

Word2Vec Overall Accuracy: 22.79%
  semantic: 4.72%
  syntactic: 37.81%
Processing FastText: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [10:23<00:00, 31.35it/s] 

FastText Overall Accuracy: 27.70%
  semantic: 3.77%
  syntactic: 47.59%

Results for ratio 0.1:
  Word2Vec: 22.79%
  FastText: 27.70%

======================================================================
 EXPERIMENT: Sample Ratio = 0.2 (20.0% of data)
======================================================================
WordEmbeddingTrainer initialized with config:
  Vector size: 300
  Workers: 20
  Epochs: 5

============================================================
Starting Complete Training Pipeline
============================================================

Step 1: Data Preparation
Sampling 20.0% of Wikipedia articles...
Sampled 1,124,731 lines from 5,623,655 total lines

Step 2: Training Word2Vec

============================================================
Training Word2Vec Model
============================================================
Sampled wiki file already exists (1,124,731 lines)
Preparing corpus for WORD2VEC...
Processing for word2vec: 1124731it [03:20, 5605.44it/s]
Corpus prepared: 1,124,731 sentences in 200.92 seconds

Training Word2Vec with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Corpus file: wiki_word2vec_corpus.txt

Word2Vec Training Complete:
  Training time: 6618.17 seconds
  Vocabulary size: 1,080,153
  Model saved to: models/word2vec_optimized.model

Sample Analogy Tests for WORD2VEC:
  man:woman :: king:? -> ['queen', 'regnant', 'princess']
  paris:france :: london:? -> ['england', 'britain', 'scotland']
  good:better :: bad:? -> ['worse', 'boujee', 'deffer']

Step 3: Training FastText

============================================================
Training FastText Model
============================================================
Sampled wiki file already exists (1,124,731 lines)
Preparing corpus for FASTTEXT...
Processing for fasttext: 1124731it [02:28, 7568.76it/s] 
Corpus prepared: 1,124,731 sentences in 148.64 seconds

Training FastText with:
  Vector size: 300
  Window: 5
  Workers: 20
  Epochs: 5
  Character n-grams: 3-6
  Corpus file: wiki_fasttext_corpus.txt

FastText Training Complete:
  Training time: 10336.16 seconds
  Vocabulary size: 1,080,191
  Model saved to: models/fasttext_optimized.model

Sample Analogy Tests for FASTTEXT:
  man:woman :: king:? -> ['regnant', 'queen', 'princess']
  paris:france :: london:? -> ['england', 'rfclondon', 'britain']
  good:better :: bad:? -> ['worse', 'betters', 'bettering']

Step 4: Model Comparison

============================================================
Model Comparison
============================================================
         Training Time (s) Vocabulary Size Vector Size Epochs
word2vec           6618.17       1,080,153         300      5
fasttext          10336.16       1,080,191         300      5

Out-of-Vocabulary (OOV) Handling Test:
  'xyzabc123': Word2Vec=False, FastText=True
  'unknownword999': Word2Vec=False, FastText=True
  'testingoov': Word2Vec=False, FastText=True

Step 5: Saving Results

Experiment results saved to: experiments/experiment_20250917_175256.json

============================================================
Total Training Pipeline Completed in 18765.13 seconds
  Word2Vec: 6618.17s
  FastText: 10336.16s
============================================================

Evaluating models for ratio=0.2...
Processing Word2Vec: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [08:09<00:00, 39.91it/s]

Word2Vec Overall Accuracy: 25.08%
  semantic: 4.83%
  syntactic: 41.91%
Processing FastText: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████| 19544/19544 [15:56<00:00, 20.44it/s]

FastText Overall Accuracy: 28.36%
  semantic: 4.14%
  syntactic: 48.49%

Results for ratio 0.2:
  Word2Vec: 25.08%
  FastText: 28.36%

Results saved to batch_results_20250917_233138.json