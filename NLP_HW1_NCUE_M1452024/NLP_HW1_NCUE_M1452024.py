# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1mYmo5BJb04eEFTX755qyrwG_oDseetga

## Part I: Data Pre-processing
"""

## Global imports
import pandas as pd
import numpy as np
import gensim.downloader
from tqdm import tqdm
import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
import os
import random
import re
import string
from gensim.models import Word2Vec, FastText
import pickle
from collections import Counter
import time
import json
from datetime import datetime
from typing import Dict, List, Tuple, Optional
##Todo 5 class import

from todo5_trainer import Todo5WordEmbeddingTrainer

## Global configuration
DATA_FILE = "questions-words.txt"
CSV_FILE = "questions-words.csv"
MODELS_DIR = "models"

def ensure_models_dir(): # Ensure the models directory exists
    
    os.makedirs(MODELS_DIR, exist_ok=True) 

# Download the Google Analogy dataset
#!wget http://download.tensorflow.org/data/questions-words.txt #operat in local

def save_model_data(model, model_name="fasttext_model", save_dir="saved_models"):
    """
    Args:
        model: 
        model_name: 
        save_dir: 
    """
    # check exist
    os.makedirs(save_dir, exist_ok=True)
    
    # save model
    model_path = os.path.join(save_dir, f"{model_name}.pkl")
    
    with open(model_path, 'wb') as f:
        pickle.dump(model, f)
    
    print(f"Model saved to: {model_path}")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")
    
    return model_path


def load_saved_model(model_path="saved_models/fasttext_model.pkl"):
    """
    load save data
    """
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")
    
    with open(model_path, 'rb') as f:
        model = pickle.load(f)
    
    print(f"Model loaded from: {model_path}")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")
    
    return model

def combine_wikipedia_files():
    """
    Combine txt files into wiki_texts_combined.txt
    """
    print("Combining Wikipedia files...")
    
    with open("wiki_texts_combined.txt", "w", encoding="utf-8") as combined_file:
        for i in range(11):  # 0 to 10
            txt_file = f"wiki_texts_part_{i}.txt"
            if os.path.exists(txt_file):
                with open(txt_file, "r", encoding="utf-8") as f:
                    combined_file.write(f.read())
                print(f"Added {txt_file}")
            else:
                print(f"Warning: {txt_file} not found")
    
    # Check the combined file
    with open("wiki_texts_combined.txt", "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    print(f"Combined file created with {len(lines)} total lines")
    print("First 5 lines:")
    for i, line in enumerate(lines[:5]):
        print(f"{i+1}: {line.strip()[:100]}...")

    
    # Check the combined file
    with open("wiki_texts_combined.txt", "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    print(f"Combined file created with {len(lines)} total lines")
    print("First 5 lines:")
    for i, line in enumerate(lines[:5]):
        print(f"{i+1}: {line.strip()[:100]}...")



def todo1_process_data_to_dataframe():
    """
    Returns:
        pd.DataFrame: DataFrame with Question, Category, SubCategory columns

    """
    
    file_name = "questions-words"
    with open(f"{file_name}.txt", "r") as f:
        data = f.read().splitlines()

    # check data from the first 10 entries
    for entry in data[:10]:
        print(entry)

    # TODO1: Write your code here for processing data to pd.DataFrame
    # Please note that the first five mentions of ": " indicate `semantic`,
    # and the remaining nine belong to the `syntactic` category.
    
    questions = []
    categories = []
    sub_categories = []
    
    current_category = None
    current_sub_category = None
    semantic_count = 0
    
    for line in data:
        line = line.strip()
        
        # Skip empty lines
        if not line:
            continue
        
        # Check if this is a category/subcategory header (starts with ':')
        if line.startswith(':'):
            current_sub_category = line
            
            # Determine if this is semantic or syntactic
            # First 5 categories are semantic, remaining are syntactic
            semantic_count += 1
            if semantic_count <= 5:
                current_category = "semantic"
            else:
                current_category = "syntactic"
        else:
            # This is an analogy question line
            words = line.split()
            if len(words) == 4:
                # Format: word_a word_b word_c word_d (a:b :: c:d)
                analogy_question = ' '.join(words)
                questions.append(analogy_question)
                categories.append(current_category)
                sub_categories.append(current_sub_category)

    # Create the dataframe
    df = pd.DataFrame(
        {
            "Question": questions,
            "Category": categories,
            "SubCategory": sub_categories,
        }
    )
    
    print(f"\nDataFrame Info:")
    print(f"Total questions: {len(df)}")
    print(f"Categories distribution:")
    print(df['Category'].value_counts())
    
    df.head()

    df.to_csv(f"{file_name}.csv", index=False)
    
    print(f"\nDataFrame saved to {file_name}.csv")
    
    return df


"""## Part II: Use pre-trained word embeddings
- After finish Part I, you can run Part II code blocks only.
"""

def todo2_predict_with_pretrained():
    """
    Uses FastText pre-trained model to perform word analogy predictions

    """
    
    # Load the processed data
    data = pd.read_csv("questions-words.csv")

    MODEL_NAME = "fasttext-wiki-news-subwords-300"
    # You can try other models.
    # https://radimrehurek.com/gensim/models/word2vec.html#pretrained-models

    # Load the pre-trained model (using FastText vectors here)
    model = gensim.downloader.load(MODEL_NAME)
    print("The Gensim model loaded successfully!")
    print(f"Model vocabulary size: {len(model.key_to_index):,}")

    # Do predictions and preserve the gold answers (word_D)
    preds = []
    golds = []

    for analogy in tqdm(data["Question"], desc="Processing analogies"):
        # TODO2: Write your code here to use pre-trained word embeddings for getting predictions of the analogy task.
        # You should also preserve the gold answers during iterations for evaluations later.
        """ Hints
        # Unpack the analogy (e.g., "man", "woman", "king", "queen")
        # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
        # Source: https://github.com/piskvorky/gensim/blob/develop/gensim/models/keyedvectors.py#L776
        # Mikolov et al., 2013: big - biggest and small - smallest
        # Mikolov et al., 2013: X = vector(”biggest”) − vector(”big”) + vector(”small”).
        """
        
        # Unpack the analogy
        words = analogy.split()
        
        if len(words) == 4:
            words_lower = [w.lower() for w in words]
            word_a, word_b, word_c, word_d = words_lower
            
            # Store the gold answer (word_d)
            golds.append(word_d)
            
            try:
                # Perform vector arithmetic: word_b + word_c - word_a should be close to word_d
                # So: vector(word_b) - vector(word_a) + vector(word_c) ≈ vector(word_d)
                # FastText can handle OOV words, so we don't need vocabulary check
                
                similar_words = model.most_similar(
                    positive=[word_b, word_c],  # Add these vectors
                    negative=[word_a],          # Subtract this vector
                    topn=1                     # Return most similar word
                )
                
                # Get the most similar word
                predicted_word = similar_words[0][0]
                preds.append(predicted_word)
                
            except Exception as e:
                # Handle any exceptions (though FastText rarely fails)
                preds.append("")
        else:
            # If the analogy doesn't have exactly 4 words, append empty prediction
            preds.append("")
            golds.append("")

    # Print basic statistics
    successful_preds = sum(1 for pred in preds if pred != "")
    print(f"\nPrediction Statistics:")
    print(f"Total questions: {len(preds)}")
    print(f"Successful predictions: {successful_preds} / {len(preds)} ({successful_preds/len(preds)*100:.2f}%)")
    print(f"Sample predictions: {[pred for pred in preds[:10] if pred != ''][:5]}")
    print(f"Sample gold answers: {golds[:10]}")

    # Perform evaluations. You do not need to modify this block!!
    def calculate_accuracy(gold: np.ndarray, pred: np.ndarray) -> float:
        return np.mean(gold == pred)

    golds_np, preds_np = np.array(golds), np.array(preds)
    
    print(f"\n=== EVALUATION RESULTS ===")
    
    # Overall accuracy
    overall_acc = calculate_accuracy(golds_np, preds_np)
    print(f"Overall Accuracy: {overall_acc * 100:.2f}%")

    # Evaluation: categories
    print(f"\nCategory-wise Accuracy:")
    for category in data["Category"].unique():
        mask = data["Category"] == category
        golds_cat, preds_cat = golds_np[mask], preds_np[mask]
        acc_cat = calculate_accuracy(golds_cat, preds_cat)
        print(f"Category: {category}, Accuracy: {acc_cat * 100:.2f}%")

    # Evaluation: sub-categories
    print(f"\nSub-category Accuracy:")
    for sub_category in data["SubCategory"].unique():
        mask = data["SubCategory"] == sub_category
        golds_subcat, preds_subcat = golds_np[mask], preds_np[mask]
        acc_subcat = calculate_accuracy(golds_subcat, preds_subcat)
        print(f"Sub-Category {sub_category}, Accuracy: {acc_subcat * 100:.2f}%")
    
    return preds, golds, model

# save the model for TODO3 usage
#model_path = save_model_data(model)
#print("Model data saved for TODO3 usage")

def todo3_plot_tsne_family():
    """
    TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
    """
    
    # Load saved model and data
    with open("saved_models/fasttext_model.pkl", 'rb') as f:
        model = pickle.load(f)
    data = pd.read_csv("questions-words.csv")
    
    # Collect words from Google Analogy dataset
    SUB_CATEGORY = ": family"
    
    # TODO3: Plot t-SNE for the words in the SUB_CATEGORY `: family`
    
    # Get family questions and extract words
    family_questions = data[data["SubCategory"] == SUB_CATEGORY]["Question"]
    family_words = set()
    for question in family_questions:
        words = question.split()
        if len(words) == 4:
            family_words.update(words)
    
    # Get word vectors
    word_vectors = []
    word_labels = []
    for word in family_words:
        try:
            word_vectors.append(model[word])
            word_labels.append(word)
        except KeyError:
            continue
    
    # Apply t-SNE
    vectors_matrix = np.array(word_vectors)
    tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_labels)-1))
    vectors_2d = tsne.fit_transform(vectors_matrix)
    
    # Plot
    plt.figure(figsize=(10, 8))
    plt.scatter(vectors_2d[:, 0], vectors_2d[:, 1])
    
    for i, word in enumerate(word_labels):
        plt.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), xytext=(5, 5), 
                    textcoords='offset points', fontsize=10)
    
    plt.title("Word Relationships from Google Analogy Task")
    plt.show()
    plt.savefig("word_relationships.png", bbox_inches="tight")



### Part III: Train your own word embeddings

### Get the latest English Wikipedia articles and do sampling.
#- Usually, we start from Wikipedia dump (https://dumps.wikimedia.org/enwiki/latest/enwiki-latest-pages-articles.xml.bz2). However, the downloading step will take very long. Also, the cleaning step for the Wikipedia corpus ([`gensim.corpora.wikicorpus.WikiCorpus`](https://radimrehurek.com/gensim/corpora/wikicorpus.html#gensim.corpora.wikicorpus.WikiCorpus)) will take much time. Therefore, we provide cleaned files for you.



def todo4_sample_wikipedia(sample_ratio=0.2):
    """
    TODO4: Sample 20% Wikipedia articles
    using wiki_sample.txt for trianing (20% Data)
    """
    print(f"TODO4: Sampling {sample_ratio*100}% of Wikipedia articles")
    
    random.seed(42)  # For reproducibility
    
    wiki_txt_path = "wiki_texts_combined.txt"
    output_path = "wiki_sample.txt"
    
    with open(wiki_txt_path, "r", encoding="utf-8") as f:
        lines = f.readlines()
    
    # TODO4: Sample 20% Wikipedia articles
    total_lines = len(lines)
    sample_size = int(total_lines * sample_ratio)
    sampled_lines = random.sample(lines, sample_size)
    
    with open(output_path, "w", encoding="utf-8") as output_file:
        output_file.writelines(sampled_lines)
    
    print(f"Original articles: {total_lines:,}")
    print(f"Sampled articles: {len(sampled_lines):,}")
    print(f"Sample saved to: {output_path}")
    
    return output_path


# TODO5: Train your own word embeddings with the sampled articles
# https://radimrehurek.com/gensim/models/word2vec.html#gensim.models.word2vec.Word2Vec
# Hint: You should perform some pre-processing before training.
# See the code in todo5_trainer.py



def todo6_predict_with_custom_embeddings():
    """
    TODO6: Use trained word embeddings for analogy predictions
    """
    # Load data
    data = pd.read_csv("questions-words.csv")
    
    # Load trained models
    word2vec_model = Word2Vec.load("models/word2vec_optimized.model")
    fasttext_model = FastText.load("models/fasttext_optimized.model")
    
    results = {}
    
    for model_name, model in [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]:
        preds = []
        golds = []
        
        for analogy in tqdm(data["Question"], desc=f"Processing {model_name}"):
            words = analogy.split()
            if len(words) == 4:
                words_lower = [w.lower() for w in words]
                word_a, word_b, word_c, word_d = words_lower
                golds.append(word_d)
                
                try:
                    # Check if words in vocabulary (Word2Vec will fail, FastText won't)
                    if model_name == "Word2Vec":
                        if all(w in model.wv for w in [word_a, word_b, word_c]):
                            result = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
                            preds.append(result[0][0])
                        else:
                            preds.append("")
                    else:  # FastText can handle OOV
                        result = model.wv.most_similar(positive=[word_b, word_c], negative=[word_a], topn=1)
                        preds.append(result[0][0])
                except:
                    preds.append("")
            else:
                preds.append("")
                golds.append("")
        
        # Calculate accuracy
        golds_np, preds_np = np.array(golds), np.array(preds)
        overall_acc = np.mean(golds_np == preds_np)
        
        results[model_name] = {
            'predictions': preds,
            'golds': golds,
            'accuracy': overall_acc
        }
        
        print(f"\n{model_name} Overall Accuracy: {overall_acc * 100:.2f}%")
        
        # Category-wise accuracy
        for category in data["Category"].unique():
            mask = data["Category"] == category
            acc = np.mean(golds_np[mask] == preds_np[mask])
            print(f"  {category}: {acc * 100:.2f}%")
    
    return results

# TODO7: Plot t-SNE for the words in the SUB_CATEGORY `: family`


def todo7_plot_tsne_custom_embeddings():
    """
    TODO7: Plot t-SNE for family words using custom embeddings
    """
    # Load models
    word2vec_model = Word2Vec.load("models/word2vec_optimized.model")
    fasttext_model = FastText.load("models/fasttext_optimized.model")
    
    # Load data
    data = pd.read_csv("questions-words.csv")
    SUB_CATEGORY = ": family"
    
    # Get family words
    family_questions = data[data["SubCategory"] == SUB_CATEGORY]["Question"]
    family_words = set()
    for question in family_questions:
        words = question.split()
        if len(words) == 4:
            words_lower = [w.lower() for w in words]
            family_words.update(words_lower)
    
    # Create plots for both models
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))
    
    for ax, (model_name, model) in zip([ax1, ax2], [("Word2Vec", word2vec_model), ("FastText", fasttext_model)]):
        word_vectors = []
        word_labels = []
        
        for word in family_words:
            try:
                if model_name == "Word2Vec" and word not in model.wv:
                    continue
                word_vectors.append(model.wv[word])
                word_labels.append(word)
            except:
                continue
        
        if len(word_vectors) > 2:
            vectors_matrix = np.array(word_vectors)
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, len(word_labels)-1))
            vectors_2d = tsne.fit_transform(vectors_matrix)
            
            ax.scatter(vectors_2d[:, 0], vectors_2d[:, 1])
            for i, word in enumerate(word_labels):
                ax.annotate(word, (vectors_2d[i, 0], vectors_2d[i, 1]), 
                          xytext=(5, 5), textcoords='offset points', fontsize=9)
            
            ax.set_title(f"{model_name} - Family Word Relationships")
            ax.set_xlabel("t-SNE dimension 1")
            ax.set_ylabel("t-SNE dimension 2")
    
    plt.suptitle("Word Relationships from Google Analogy Task")
    plt.tight_layout()
    plt.savefig("word_relationships_custom.png", bbox_inches="tight")
    plt.show()
    
    print(f"Plot saved as word_relationships_custom.png")


if __name__ == "__main__":
    
    # TODO1: Process data
    df = todo1_process_data_to_dataframe()
    
    # TODO2-3: Pre-trained model
    preds, golds, model = todo2_predict_with_pretrained()
    todo3_plot_tsne_family()
    
    # TODO4-5: Training
    todo4_sample_wikipedia()
    trainer = Todo5WordEmbeddingTrainer()
    trainer.run_complete_training()
    
    # TODO6-7: Custom model evaluation
    results = todo6_predict_with_custom_embeddings()
    todo7_plot_tsne_custom_embeddings()

    
    
